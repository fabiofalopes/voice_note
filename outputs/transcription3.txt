{
  "text": " This tiny computer is a Raspberry Pi. It's made for schools and loved by makers and more specifically this is the Raspberry Pi 5 which was released a few months ago. Now this version is an 8 gigabyte of RAM model, costs just 80 pounds in the UK or 80 dollars in the US if you're lucky enough to be able to get hold of one. So this tiny computer can be used for many things but specifically in this video I want to show you how you can use that 8 gigabyte of RAM for running an open source large language model on your own network and what sort of benchmarks we can get versus say something like the MacBook Pro that I've used Alarma on in the past. So with that all said let's get started. So I'm on my Pi 5. I'm going to try and install Alarma on this and see how it goes. I should be able to run the Alarma instructions and just see how they pan out so if we just copy that curl commands and paste that see how that does okay cool so that seems to just gone in and install straight away so if you're not familiar with Alarma you can go and pick up any of these models it's got listed here so you got Mixtral, you got Lama 2, Tiny I'm just going to try and run TinyLama at this point. We can just run TinyLama and it'll pull down that model. So let's see how we do. I've never run TinyLama before, so this is going to be a new one for me. So I'm running Raspbian, as you can see, and I've updated everything, installed all the latest packages, and I haven't installed anything else. I've literally just installed a llama there. Okay, cool, so it's pulled down everything. Let's have a question, classic, why is the sky blue? See how that does. Sky blue is a natural color. Why is the sky blue? Oh, that's interesting the way it's phrased that. I'm guessing it's just basically down because it's a tiny llama, which is not as big model as other options. Okay, so it's actually worked, which is superb. So I'm actually pretty surprised that that got installed so quickly and was so easy I going to try out a few things the fan did kick in on the heat heatsink there when I was trying things So it is obviously using the CPU a bit It would be interesting to know when we do this So we can run this verbose command. So if I do a llama run tiny llama dash dash verbose, I think it is. Then when we do y, do the same thing. we should get some stats out in terms of how fast it's generating those responses. Now, when I was doing this on my M1 Pro, on Llama, not Tiny Llama, we're getting about 22 a second, I think. And on my M1, I think it was like 17 or something like that. So eval rate 12.9 tokens a second. that is prompt eval so the eval rate is 10 tokens, so roughly half what I was getting on the M1 program, which is not too shabby, we could actually do a better comparison if we pulled down the other model so if I say buy and come out, and then do Lama run Lama2 I'm actually going to pull down the uncensored one because because Lama 2 is pretty restrictive. It's pretty aggressive with the restrictions it applies to. You could ask for a really spicy sauce, or I think in my other video I asked for a regex in Python, and it wouldn't give me the answer to those regexes because it felt that they were inappropriate and that I might be trying to do nefarious things with them. So this is saying it's going to take about 10 minutes. So that's obviously a 4 gig model now. Just wait a second and let that pull that down. OK, cool. That's all finished downloading as well as the Lama 2 uncensored model. I've pulled down Lava as well because I wanted to check out if it can do how well it copes with doing image kind of interpretations. So let first run the Lama 2 uncensored and see how that fares And in fact actually let do that with the verb post command again So I'm going to prompt it with, can you write a regular expression to match email addresses? addresses. So in previous video when I did this it actually this is the reason for using the uncensored version because then this doesn't get caught. Like I said things is a little overzealous stuff and that generally is to do with the initial system prompt. You can see that this is much slower than the tiny llama that we were running okay so it's doing in JavaScript I didn't actually specify that one in Python but there we go that's fine I have no idea if that's gonna match any more address well this is yeah this is this is really slow comparison so you probably want to be using one those smaller models so yeah this is the 7 billion parameter model I didn't state that but it says on the Lama website under the uncensored the llama 2 model that the memory requirements are that 7 billion parameter models require generally 8 gig RAM which we've got here but you can see that it's not it's not fast okay yeah so you can see there we've gotten if if our rate of 1.78 so tiny in comparison to what we had just now with the tiny llama so obviously the model is that much more bigger is double size we've gone from 1.78 to 3.78 I think that's a 3 billion parameter model let me have a squiz of the website in fact actually no it's a 1.1 billion parameter model which is obviously a lot smaller we're going from 7 1.1 billion to 7 billion and we're getting much slower eval rate so this is probably not the way you want to go you probably want to be using something like Mistral on this or in fact Tiny Llama is a good option there because we seem to be going pretty fast I going to try this image as well So I download this image into downloads It is a picture of the Raspberry Pi Let me see if I can get it to understand that because that would be pretty awesome to know that it can do that as well. So let's run lava and we're going to run that verbose as well. Man, I've got an absolute tweet storm going on in a tree in my garden. This happens all the time. Okay, so let's see. What's in this picture? Home in downloads image.jpg. I think that's what it was called. Okay, let's go. Wow, this is slow and you've got no feedback is the other thing here. We're not seeing anything aside from a spinny snake. And it's finally responding with an answer. Here we go. The image features a close-up of the back of a computer circuit board. Green and yellow. The board has many screws on it, attaching various components detailed view showcases inner workings of electronic devices such as laptops or computers so it's obviously looked at that image and it understands it and it's done all that locally which is really impressive it's not gone out to a third party service in order to do that it hasn't been able to pick anything out from the image file name because I've made sure that it's not identifiable from what I've named the file so that's really impressive but is incredibly slow it took how long did that take total duration five minutes 33 so a long time we've obviously got all of the features that um alarma has as well such as the api stuff you can go and check my previous videos if you want to see how to do that but yeah i hope you found this useful uh let me know if you're going to be trying it out on your own raspberry pi i'll speak to you soon in a new video and check out one of my other videos on alarma there'll be one popping up in a minute probably. Okay, bye for now. Bye.",
  "summary": "The video showcases the capabilities of the Raspberry Pi 5, a tiny computer, in running large language models. The author installs Alarma, an open-source language model, on the Raspberry Pi 5 with 8GB of RAM. They test the model's performance by running TinyLama, a smaller model, and Lama 2, a larger model with 7 billion parameters. The results show that TinyLama performs relatively well, with an evaluation rate of 12.9 tokens per second, while Lama 2 is much slower, with an evaluation rate of 1.78 tokens per second. The author also tests Lava, an image interpretation model, which is able to correctly identify the contents of an image, but is extremely slow, taking over 5 minutes to respond. The video demonstrates the potential of the Raspberry Pi 5 in running complex AI models, but also highlights the limitations of its processing power.",
  "sentiment_analysis": {
    "sentiment_analysis": {
      "sentiment": "positive",
      "confidence_score": 0.85
    }
  }
}